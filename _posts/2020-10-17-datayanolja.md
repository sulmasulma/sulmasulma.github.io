---
layout: post
title: 데이터야놀자 2019
tags: [seminar]
categories: [Etc]
excerpt_separator: <!--more-->
---
2020 [데이터야놀자](https://datayanolja.github.io)에서 듣고 메모한 내용을 적어 보았습니다.<!--more-->

![schedule](https://eventusstorage.blob.core.windows.net/evs/Image/datayanolja2020/23268/ProjectInfo/d786f217133f4c998230995b2688563a.png)

---

### 1. 당근마켓이 데이터와 함께 노는 법

#### 모델 학습 및 서빙

- 머신러닝 모델을 통한 글 분류
- 게시글의 텍스트를 활용하여 필터링
- 동네생활: 4만 개, 중고거래: 40만 개 정도의 글로 학습
- 사람같은 경우 텍스틀 이해하고 글 판단. 머신러닝 모델은 숫자를 입력으로 받기 때문에, 텍스트를 tokenize하여 숫자로 바꿔야 함
- 구글에서 개발한 `sentencepiece` 라이브러리 활용: 속도가 월등함
- NLP에서 baseline이 된 `BERT` 사용
  - 적은 데이터로 높은 성능 낼 수 있음
  - 라벨링되지 않은 데이터로 사전 학습?
  - `transformers` 라이브러리 사용
- 데이터의 불균형 문제
  - 필터링되지 않아도 되는 정상적인 게시글이 대부분 차지
  - 클래스별 불균형이 큼
- 해결책
  - loss function 변형 (focal loss-비전 분야에서 많이 사용-등)
  - oversampling: focal loss 사용시보다 높은 성능
- F1 score 0.84
  - 카테고리별 F1 score의 평균 수치
  - 특정 카테고리 비율이 낮기 때문. 후처리하면 더 성능 높게 나옴
- model server
  - `flask`도 가능
  - 머신러닝 모델에 특정하게 필요한 기능이 필요
  - pytorch의 `torch-serve` 사용: 여러 API 제공, 모델 버전 관리 등 운영에 편리한 요소 있음
  - flask보다 더 좋은 성능 + 낮은 cpu, memory 사용
- 이런 식으로 모델 학습 + 서빙하고 있음

#### 데이터 인프라

- 1000만 MAU -> 빠르고 안전하게 데이터를 저장하고, 쉽고 빠르게 꺼낼 수 있어야 함
  - 데이터 민주화(Democratization): 접근성 높임
  - 빠르게 꺼낼 수 있게
  - 안전하게: 사용자별 접근 권한 제어, 유실되지 않고 안전하게 저장
- **ELT**, not ETL
  - 클라우드로 인해 데이터의 저장/처리 비용이 싸고 빨라져서 가능해짐
  - 문제: 저장하기 전에 변환해야 함. 근데 이러면 저장 형태를 정하는 시간이 오래 걸림. 결과적으로 저장하는 데 시간이 오래 걸림
  - 그래서 저장해 놓고, 꺼내 쓸 때 처리
  - 예쁘게 쌓는 것보다, 일단 쌓고 활용하는 것이 중요
- Lambda Architecture
  - 2트랙
    - 배치 프로세스를 이용하여 작은 단위로 데이터를 쪼개서 주기적으로 처리
    - 스트리밍 데이터로 실시간 처리
  - 과거의 히스토리컬 데이터부터 실시간 데이터까지 처리할 수 있음
- `BigQuery`
  - 메인 DW (DL에 가깝긴 함)
  - **편리한 웹 UI** -> 일반 사용자들의 접근성 높아짐
  - 저렴한 비용과 빠른 조회. 처리 시간이 아닌 **스캔한 데이터의 양** 으로만 과금
    - 쿼리를 정교하게 짜지 않아도, 결과 데이터의 양만 신경 쓰면 됨
  - 보안 기능도 제공
  - BigQuery omni: 빅쿼리 인프라를 AWS 같은 다른 클라우드에서 사용할 수 있는 것?
- 고급 분석
  - `S3`, `Athena`, `Spark`, `Zeppelin/Jupyter` 등 사용
- 셀프 BI 시스템
  - Firebase를 사용하고 있었는데, 사용자와 데이터가 많아지다 보니 BigQuery의 문제 발생
  - 범용성으로 인해 데이터 저장 시점이 늦어지고, 데이터 포맷 지정할 수 없는 등의 문제
  - 그래서 프로젝트 진행 중
- 파이프라인 설계 과정에서 티어(단계)별 데이터 정리
  - 다음 단계에서는 전 단계에서 나온 데이터를 모두 사용하지는 않음
  - 필요에 따라 가공된 테이블을 미리 생성해 둠
- 기술 발전 속도가 성장 속도를 따라가지 못하고 있는 상태. 그래서 지속적으로 발전+채용 중

<br>

---

### 2. 오픈소스 기여: 코드로 세상에 기여하는 또다른 방법

- 존경하는 개발자들이 만든 거대한 오픈소스 프로젝트(e.g. `tensorflow`)에 관심을 가지게 됨
- 첫 오픈소스 경험. 실패담
  - 무작정 오픈소스 프로젝트를 찾아서 코딩했었음. 코드를 수정하고 압축해서 메일로 보냄. 그리고 무시당함
  - `git`을 통한 버전관리도 모를 시절
- 개인 프로젝트의 한계 체감
  - 실력 향상에 대한 의구심이 들어, 고수들의 코드를 보고 싶었음
- 오픈소스 경진대회 참가 (공개 SW 컨트리뷰톤)
  - 굵직한 프로젝트, 코드 난이도가 높은 편
  - 통째로 취약점 진단 툴로 돌린다면? -> 효과 없었음
  - 유저의 입장으로서 프로젝트 분석 시작. 영문 문서를 꼼꼼히 읽고, 모든 기능과 원리 파악
  - use case로서의 컨트리뷰션
- 문서화 작업을 하며, maintainer의 꼼꼼한 검토 받음
  - 문서화를 위해서는 꼼꼼한 user로서의 경험이 필요
  - 이를 통해 code commit 외의 기여(문서 기여 + user case 기여)
- contribute 할 만한 프로젝트
  - 최근까지 활발하게 논의되고 있는 프로젝트. 즉 issue나 maintainer의 comment가 최근일 경우
- git의 용도
  - 개인 프로젝트에서는 백업 용도이지만, 오픈소스 프로젝트에서는 다름
- 오픈소스에 code 기여하려면, 프로젝트를 모두 알아야 할까?
  - 쉬운 contibution은 issue에서 실마리를 찾을 수 있음. **issue를 해결할 수 있을 정도면 됨**
  - `GitHub`에 있는 issue들 중 해결 가능한 것 찾기. issue도 분류가 되어 있음 (버그, 기능 향상, Good First Issue(쉬운 이슈) 등)
  - 실력이 압도적으로 향상된 이후에 기여하기보다는, 작은 이슈부터 해결하면서 성장하는 것이 바람직
- 오픈소스 프로젝트 기여의 이점
  - 채용시 우대
  - **높은 수준의 개발자들과 논의하면서 협업 경험**
  - **정교한 코드** 를 작성하는 경험
  - SW Tester로서의 안목
  - 영어 실력

<br>

---

### 3. Kubernetes와 함께 데이터 흐름을 더 나이스하게 바꾸기

- 데브시스터즈 플랫폼셀의 데이터 엔지니어
- 데이터 플랫폼에서 `Kubernetes`의 이점
- 데이터 플랫폼
  - 사내 데이터 수집, 적재, 가공하여 서비스
  - 쿠키런에서 발생하는 데이터를 `Kafka`를 통해 수집, `ELK stack`을 통해 실시간 데이터 시각화
  - 데이터는 parquet 형태로 `S3`(cloud object storage)에 저장
  - 복잡한 데이터는 `pyspark` + `Jupyter` 사용
  - 데이터가 흐르고는 있지만, **나이스하게 흐르고 있을까?** 에 대한 의문
- `Kibana`에서 전문가가 아니더라도 용도에 따른 데이터 시각화 가능
  - Kibana만으로는 모든 목적을 달성할 수 없음
  - 오래된 데이터는 삭제하고 일정 기간의 데이터만 저장
  - 고급 집계와 정확한 집계 불가능
- S3에 모든 데이터 저장
  - 오래된 데이터는 유실되는게 아니라, S3에 있음
  - DL, 가공된 DW 등도 모두 S3에 저장
  - 여기에 모든 데이터가 있긴 한데, 모든 구성원에게 나이스하게 흐르고 있지 못함
- 대시보드: `Athena` + BI Tool(Mode Analytics)
- 데이터 분석, 추출: `pyspark` + `Jupyter`
- 이게 나이스하지 않음
- Athena
  - Scan당 비용. Full Scan을 남발하면 비용 폭발 (지난달 대비 5천불 증가한 적도 있음)
  - 성능이 좋지 않음. 느린 쿼리를 최적화할 수 있는 여지가 적음
  - On Demand가 아님(성능 옵션이 없음). 쿼리 개선만 가능
  - Mode Analytics는 사용자당 비용. Athena는 시각화 툴이 아니기 때문에, 시각화 기능 생각해야 함
- `Spark`로 천하통일?
  - Spark Thrift Server: JDBC로 쿼리 가능(Spark SQL)
  - `flintrock`이라는 CLI 사용
    - Spark 클러스터 생성 툴
    - CLI를 웹에 래핑해서 사용
  - 기존에는 데이터 과학자 등 특정 구성원만 사용. 근데 이제는 모든 사용자에게 접근 가능해야 함
- `Kubernetes`
  - Spark 2.3부터 native하게 지원. 3.0에서는 동적 리소스 할당도 지원
- Spark on Kubernetes
  - 이 부분은 Spark와 Kubernetes를 좀 더 이해하고 읽어야 할 듯(Pod 개념)
  - Cluster Autoscaler
- 시각화는 범용성이 중요 -> `Superset`
  - Spark Thrift Server + Superset: 비개발 직군에서 오는 데이터 추출 작업 요청에 대해 Superset으로 SQL 작성 후 공유. 업무 부담 적어짐
- 일반 사용자는 개선됐지만, 분석가의 데이터 분석 환경은 괜찮은가?
  - 이 경우도 Kubernetes 사용
  - Jupyterhub: 각 사용자가 웹에서 Jupyter 요청하면, 사용자별 Spark 서버 사용. Kubernetes가 자동으로 Pod 사용하여 사용자별 할당. Jupyter 사용시 로컬이 아닌 원격 서버 사용하는 느낌인 듯
- Spark Thrift Server를 Kubernetes 상에서 사용
  - 사용량에 따른 동적 할당
  - JDBC를 이용해서 모든 데이터 분석
  - ETL pipeline도 Kubernetes에서 사용
- 궁극적인 이점
  - 잡다한 문제 대신 데이터 분석에 집중
  - 본연의 미션, 가치에 몰입할 수 있게 됨

<br>

---

### 4. 클라쓰가 다른 데이터 플랫폼

- 클래스101
-

<br>

---

### 5. 대학교 경제학 강의를 "힙"한 데이터 특강으로 피봇팅한 썰

-

<br>

---

### 6. 우아한 형제들에서 데이터로 일하는 방법

-

<br>

---

### 7. 좀 더 똑똑한 검색 기능을 만들어 보자

- `Elasticsearch`

<br>

---

### 8. Airflow로 똑똑한 배치관리하기

-

<br>

---
