---
layout: post
title: Zeppelin 설치
tags: [Data Engineering, Zeppelin, Install]
categories: [Etc]
excerpt_separator: <!--more-->
---
AWS에서 `EMR`(Elastic Map Reduce, 클라우드 컴퓨팅)을 통해 `Zeppelin`을 사용할 경우 실제 컴퓨팅 시간과 관계 없이 서버를 켜 놓은 시간에 비례하여 과금되기 때문에, 요금 폭탄을 맞기 십상이다.<!--more--> `S3`, `RDB` 등 데이터 저장은 프리 티어 범위 안에서 가능하지만 `EMR`은 해당되지 않는다. 그래서 `Zeppelin`을 통해 pyspark를 사용하는 것은 로컬에서 설치하여 진행하기로 했다.
- 현재 Java 설치(1.8.0_241 버전) 후 환경 변수(PATH) 설정 완료 상태

### 1. Scala 설치
- `brew install scala`
- `vi ~/.bash_profile`에 다음 커맨드 추가
  ```
  export SCALA_HOME=/usr/local/Cellar/scala/2.13.1_1/libexec/
  export PATH=$PATH:$SCALA_HOME/bin
  ```
- `source ~/.bash_profile`

### 2. Spark 설치
- `brew install apache-spark`
- `vi ~/.bash_profile`에 다음 커맨드 추가
  ```
  export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.5/libexec/
  export PATH=$PATH:$SPARK_HOME
  export SPARK_LOCAL_IP=127.0.0.1 # 로컬에서 spark를 실행시키는경우 필요한 부분.
  ```
- `source ~/.bash_profile`
- 환경 변수들 확인
  ```
  (base) Matthewui-MacBookPro:work matthew$ echo $JAVA_HOME
  /Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home/
  (base) Matthewui-MacBookPro:work matthew$ echo $SCALA_HOME
  /usr/local/Cellar/scala/2.13.1_1/libexec/
  (base) Matthewui-MacBookPro:work matthew$ echo $SPARK_HOME
  /usr/local/Cellar/apache-spark/2.4.5/libexec/
  ```
- `spark-shell` 실행
  - `localhost:4040` 에서 Spark UI 확인 가능
  - spark shell 종료: `System.exit(0)`
- `pyspark` 설치
  - `pip3 install pyspark`
  - 터미널에서 `pyspark` 명령어로 spark shell 실행
  - shell 종료는 python과 똑같이 `quit()`으로 가능

### 3. Zeppelin 설치
- Zeppelin은 현재 Homebrew로 설치할 수 없는 상태
- [Zeppelin 홈페이지](https://zeppelin.apache.org/download.html)에서 `.tgz` 파일을 다운받아 설치해도 되는데, 필자는 이 경우 빌드가 성공적으로 되지 않는다.
- 따라서 **GitHub 저장소를 clone하여 설치**
1. Clone
  - `git clone https://github.com/apache/zeppelin.git`
  - `cd folder`: 설치한 폴더로 들어가기
2. Build
  - `maven`이 설치되어 있지 않다면: `brew install maven`
  - `mvn clean package -DskipTests`
  - 이 과정이 시간이 좀 걸림. 필자는 2시간 30분
3. Run
  - `./bin/zeppelin-daemon.sh start`
    - 종료는 `./bin/zeppelin-daemon.sh stop`
  - `localhost:8080` 으로 접속
4. 예제 코드 실행
  - 노트북 생성: `Create new note` 클릭하여 `spark`로 생성
    ![스크린샷 2020-04-23 오후 12.08.11](/assets/스크린샷%202020-04-23%20오후%2012.08.11.png)
  - 코드 실행
    ![스크린샷 2020-04-23 오후 12.07.43](/assets/스크린샷%202020-04-23%20오후%2012.07.43.png)
- Zeppelin 설치 과정에서 `Scala`, `Spark` 등의 인터프리터를 모두 설치하는데, 이것을 사용하는 것인지 or 위에서 `SCALA_HOME`, `SPARK_HOME`으로 설정한 것을 사용하는 것인지 확실하지 않다.

### 4. Zeppelin에서 S3 데이터 불러오기
- `pyspark`에서 바로 AWS S3 데이터 불러올 수 없음
  - `java.io.IOException: No FileSystem for scheme: s3`
- 추가적인 configure 필요
  - https://medium.com/@mrpowers/working-with-s3-and-spark-locally-1374bb0a354
  - https://eyeballs.tistory.com/156
  - 필요시 추가적인 문서 더 찾아 완료하기
- AWS Access key 및 `Hadoop` 설치 필요한 듯

---
- 참고 문서
  - [Homebrew로 Zeppelin까지 설치하기](https://swalloow.github.io/spark-zeppelin-install)
    - 참고로 현재 Zeppelin은 `Homebrew`에서 설치 불가
  - [Homebrew로 pyspark 설치 및 샘플 코드](http://sanghun.xyz/2017/12/mac-apche-spark-%EC%84%A4%EC%B9%98/)
  - [How to Build Zeppelin from Source](https://zeppelin.apache.org/docs/latest/setup/basics/how_to_build.html): Zeppelin 공식 문서
  - [성윤님 블로그 글: Apache Zeppelin(아파치 제플린)](https://zzsza.github.io/data/2018/06/02/apache-zeppelin/)
