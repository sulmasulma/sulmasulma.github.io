---
layout: post
title: Zeppelin 설치
tags: [Data Engineering, Zeppelin, Install]
categories: [Etc]
excerpt_separator: <!--more-->
---
Zeppelin 설치<!--more-->

AWS에서 `EMR`(Elastic Map Reduce, 클라우드 컴퓨팅)을 통해 `Zeppelin`을 사용할 경우 실제 컴퓨팅 시간과 관계 없이 서버를 켜 놓은 시간에 비례하여 과금되기 때문에, 요금 폭탄을 맞기 십상이다. `S3`, `RDB` 등 데이터 저장은 프리 티어 범위 안에서 가능하지만 `EMR`은 해당되지 않는다. 그래서 `Zeppelin`을 통해 pyspark를 사용하는 것은 로컬에서 설치하여 진행하기로 했다.
<!--
- 버전 호환 이슈
  - [관련 링크](https://deep-dive-dev.tistory.com/27)
  - Zeppelin 0.8.0 version에서 Spark 2.2.3 version, Java 8 version의 조합으로만 정상적으로 동작
  - 아래 버전들로 정상적으로 작동하는지 확인해 보기
  - 몇 일 동안 삽질한지 모르겠다..

### 1. Spark 설치
- [변성윤님 Spark 인트로](https://zzsza.github.io/data/2018/05/29/apache-spark-intro/)
  - Spark 최신 버전(이 때 기준 2.3.0)으로 설치한 듯. 지금은 2.4.5가 최신
  - 2.2.3 버전은 다운 속도가 느려서 [2.4.5 버전](https://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz)으로 설치
- 압축 풀고 실행하면, Java 오류 뜸 -> 버전 호환 문제인 듯. 지금 컴퓨터에 있는 Java 버전은 14 SE이기 때문
  - `java.lang.ExceptionInInitializerError`

### 2. Java 설치
- **Spark 2.4.5는 Java 8 버전과 호환**
  - [https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/)
- Java 8 설치
  - [https://www.oracle.com/java/technologies/javase-jdk8-downloads.html](https://www.oracle.com/java/technologies/javase-jdk8-downloads.html)
- 설치 후 Java 환경 변수(PATH) 설정
  - https://khstar.tistory.com/entry/%EB%A7%A5-MacOS%EC%97%90-JAVAHOME-%ED%99%98%EA%B2%BD%EC%84%A4%EC%A0%95
- 이제 Spark 실행하면, 정상적으로 작동함 (변성윤님 인트로 따라서)
  ```
  ./bin/spark-shell
  sc.version
  ```
- **특이사항:** `Zeppelin` 다운받고 나니까 다시 안 됨!!

### 3. Zeppelin 설치
- 0.9.0-preview1 버전 설치
  - preview 버전? 아직 정식 버전이 아닌가
  - [http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.9.0-preview1/zeppelin-0.9.0-preview1-bin-all.tgz](http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.9.0-preview1/zeppelin-0.9.0-preview1-bin-all.tgz)
  - `http://mirror.apache-kr.org/zeppelin/zeppelin-0.9.0-preview1/zeppelin-0.9.0-preview1-bin-all.tgz` 클릭
- 가이드대로 설치
  - [https://zeppelin.apache.org/docs/0.9.0-preview1/quickstart/install.html#requirements](https://zeppelin.apache.org/docs/0.9.0-preview1/quickstart/install.html#requirements)
  - 빌드 중 오류 발생
    ```
    ./bin/zeppelin.sh: line 27: getent: command not found
    Container ENTRYPOINT failed to add passwd entry for anonymous UID
    Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
    ```
    - 크게 신경 쓸 필요는 없어 보이지만, 다음 과정으로 진행되지 않음
    - 출처: [https://blog.voidmainvoid.net/184](https://blog.voidmainvoid.net/184)
- 0.8.2 버전으로 다시 설치
  - 빌드 중 오류 발생
    ```
    SLF4J: Class path contains multiple SLF4J bindings.
    SLF4J: Found binding in [jar:file:/Users/matthew/Downloads/zeppelin-0.8.2-bin-all/lib/interpreter/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/Users/matthew/Downloads/zeppelin-0.8.2-bin-all/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
    ```
  - 프로세스가 정상적으로 종료되지 않은 것
  - `ps -ef | grep zeppelin` 으로 찾아서 `kill -9 프로세스번호`로 종료 -->

## 참고 문서
- `Homebrew`로 Zeppelin까지 설치하기
  - [https://swalloow.github.io/spark-zeppelin-install](https://swalloow.github.io/spark-zeppelin-install)
- Homebrew로 pyspark 설치
  - [http://sanghun.xyz/2017/12/mac-apche-spark-%EC%84%A4%EC%B9%98/](http://sanghun.xyz/2017/12/mac-apche-spark-%EC%84%A4%EC%B9%98/)
- 현재 Java 설치 후 환경 변수(PATH) 설정 완료 상태

### 1. Scala 설치
- `brew install scala`
- `vi ~/.bash_profile`에 다음 커맨드 추가
  ```
  export SCALA_HOME=/usr/local/Cellar/scala/2.13.1_1/libexec/
  export PATH=$PATH:$SCALA_HOME/bin
  ```
- `source ~/.bash_profile`

### 2. Spark 설치
- `brew install apache-spark`
- `vi ~/.bash_profile`에 다음 커맨드 추가
  ```
  export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.5/libexec/
  export PATH=$PATH:$SPARK_HOME
  export SPARK_LOCAL_IP=127.0.0.1 # 로컬에서 spark를 실행시키는경우 필요한 부분.
  ```
- `source ~/.bash_profile`
- 환경 변수들 확인
  ```
  (base) Matthewui-MacBookPro:work matthew$ echo $JAVA_HOME
  /Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home/
  (base) Matthewui-MacBookPro:work matthew$ echo $SCALA_HOME
  /usr/local/Cellar/scala/2.13.1_1/libexec/
  (base) Matthewui-MacBookPro:work matthew$ echo $SPARK_HOME
  /usr/local/Cellar/apache-spark/2.4.5/libexec/
  ```
- `spark-shell` 실행
  - `localhost:4040` 에서 Spark UI 확인 가능
  - spark shell 종료: `System.exit(0)`
- `pyspark` 설치
  - `pip3 install pyspark`
  - 터미널에서 `pyspark` 명령어로 spark shell 실행
  - 종료는 python과 똑같이 `quit()`으로 가능

### 3. Zeppelin 설치
- `Zeppelin`은 현재 Homebrew로 설치할 수 없는 상태
- [성윤님 블로그 글](https://zzsza.github.io/data/2018/06/02/apache-zeppelin/) 따라서 설치
  - 0.8.2 버전
- **빌드 시작**
  - configure 파일에서 JAVA_HOME, SPARK_HOME을 수정하는 것과 관계 없이 `SLF4J: Class path contains multiple SLF4J bindings.` 오류 발생
    - [링크]() 대로 binding 파일 하나 삭제함
  - 계속해서 오류 발생
    - `org.glassfish.jersey.internal.inject.Providers checkProviderRuntime`
  - 빌드 실패. 일단 AWS EMR로 다시 학습 시작. 학습 시에만 클러스터 생성하고 끝나면 종료하기


### 4. Zeppelin에서 S3 데이터 불러오기
- pyspark에서 바로 AWS S3 데이터 불러올 수 없음
  - `java.io.IOException: No FileSystem for scheme: s3`
- 추가적인 configure 필요
  - https://medium.com/@mrpowers/working-with-s3-and-spark-locally-1374bb0a354
  - https://eyeballs.tistory.com/156
  - 필요시 추가적인 문서 더 찾기
- AWS Access key 및 `Hadoop` 설치 필요한 듯

---
- 참고 문서 정리 필요
