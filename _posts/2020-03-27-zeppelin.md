---
layout: post
title: Zeppelin 설치
tags: [Data Engineering, Zeppelin, Install]
categories: [Etc]
excerpt_separator: <!--more-->
---
AWS에서 `EMR`(Elastic Map Reduce, 클라우드 컴퓨팅)을 통해 `Zeppelin`을 사용할 경우 실제 컴퓨팅 시간과 관계 없이 서버를 켜 놓은 시간에 비례하여 과금되기 때문에, 요금 폭탄을 맞기 십상이다.<!--more--> `S3`, `RDB` 등 데이터 저장은 프리 티어 범위 안에서 가능하지만 `EMR`은 해당되지 않는다. 그래서 `Zeppelin`을 통해 pyspark를 사용하는 것은 로컬에서 설치하여 진행하기로 했다.

### 1. Scala 설치
- `brew install scala`
- `vi ~/.bash_profile`에 다음 커맨드 추가
  ```
  export SCALA_HOME=/usr/local/Cellar/scala/2.13.1_1/libexec/
  export PATH=$PATH:$SCALA_HOME/bin
  ```
- `source ~/.bash_profile`

### 2. Spark 설치
- `brew install apache-spark`
- `vi ~/.bash_profile`에 다음 커맨드 추가
  ```
  export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.5/libexec/
  export PATH=$PATH:$SPARK_HOME
  export SPARK_LOCAL_IP=127.0.0.1 # 로컬에서 spark를 실행시키는경우 필요한 부분.
  ```
- `source ~/.bash_profile`
- 환경 변수들 확인
  ```
  (base) Matthewui-MacBookPro:work matthew$ echo $JAVA_HOME
  /Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home/
  (base) Matthewui-MacBookPro:work matthew$ echo $SCALA_HOME
  /usr/local/Cellar/scala/2.13.1_1/libexec/
  (base) Matthewui-MacBookPro:work matthew$ echo $SPARK_HOME
  /usr/local/Cellar/apache-spark/2.4.5/libexec/
  ```
- `spark-shell` 실행
  - `localhost:4040` 에서 Spark UI 확인 가능
  - spark shell 종료: `System.exit(0)`
- `pyspark` 설치
  - `pip3 install pyspark`
  - 터미널에서 `pyspark` 명령어로 spark shell 실행
  - 종료는 python과 똑같이 `quit()`으로 가능

### 3. Zeppelin 설치
- `Zeppelin`은 현재 Homebrew로 설치할 수 없는 상태
- [성윤님 블로그 글](https://zzsza.github.io/data/2018/06/02/apache-zeppelin/) 따라서 설치
  - 0.8.2 버전
- **빌드 시작**
  - configure 파일에서 JAVA_HOME, SPARK_HOME을 수정하는 것과 관계 없이 `SLF4J: Class path contains multiple SLF4J bindings.` 오류 발생
    - [링크]() 대로 binding 파일 하나 삭제함
  - 계속해서 오류 발생
    - `org.glassfish.jersey.internal.inject.Providers checkProviderRuntime`
  - 빌드 실패. 일단 AWS EMR로 다시 학습 시작. 학습 시에만 클러스터 생성하고 끝나면 종료하기


### 4. Zeppelin에서 S3 데이터 불러오기
- pyspark에서 바로 AWS S3 데이터 불러올 수 없음
  - `java.io.IOException: No FileSystem for scheme: s3`
- 추가적인 configure 필요
  - https://medium.com/@mrpowers/working-with-s3-and-spark-locally-1374bb0a354
  - https://eyeballs.tistory.com/156
  - 필요시 추가적인 문서 더 찾아 완료하기
- AWS Access key 및 `Hadoop` 설치 필요한 듯

---
- 참고 문서
  - `Homebrew`로 Zeppelin까지 설치하기
    - [https://swalloow.github.io/spark-zeppelin-install](https://swalloow.github.io/spark-zeppelin-install)
  - Homebrew로 pyspark 설치
    - [http://sanghun.xyz/2017/12/mac-apche-spark-%EC%84%A4%EC%B9%98/](http://sanghun.xyz/2017/12/mac-apche-spark-%EC%84%A4%EC%B9%98/)
  - 현재 Java 설치 후 환경 변수(PATH) 설정 완료 상태
